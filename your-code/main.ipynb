{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0HGrNOzyqt8"
   },
   "source": [
    "# Bag of Words Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Bag of words (BoW)** is an important technique in text mining and [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval). It turns the content of text into vectors of numbers which makes it possible to use mathematics and computer programs to analyze and compare documents.\n",
    "\n",
    "A BoW contains the following information:\n",
    "\n",
    "1. A dictionary of all the terms (words) in the text documents. The terms are normalized in terms of the letter case (e.g. `Ironhack` => `ironhack`), tense (e.g. `had` => `have`), singular form (e.g. `students` => `student`), etc.\n",
    "1. The number of occurrences of each normalized term in each document.\n",
    "\n",
    "For example, assume we have three text documents:\n",
    "\n",
    "DOC 1: **Ironhack is cool.**\n",
    "\n",
    "DOC 2: **I love Ironhack.**\n",
    "\n",
    "DOC 3: **I am a student at Ironhack.**\n",
    "\n",
    "The BoW of the above documents looks like below:\n",
    "\n",
    "| TERM | DOC 1 | DOC 2 | Doc 3 |\n",
    "|---|---|---|---|\n",
    "| a | 0 | 0 | 1 |\n",
    "| am | 0 | 0 | 1 |\n",
    "| at | 0 | 0 | 1 |\n",
    "| cool | 1 | 0 | 0 |\n",
    "| i | 0 | 1 | 1 |\n",
    "| ironhack | 1 | 1 | 1 |\n",
    "| is | 1 | 0 | 0 |\n",
    "| love | 0 | 1 | 0 |\n",
    "| student | 0 | 0 | 1 |\n",
    "\n",
    "\n",
    "The vector of each document in BoW can be high-dimensional since it can have as many terms as there exist words in the language. Data scientists use these vectors to represent the content of the documents. For instance, DOC 1 is represented with `[0, 0, 0, 1, 0, 1, 1, 0, 0]`, DOC 2 is represented with `[0, 0, 0, 0, 1, 1, 0, 1, 0]`, and DOC 3 is represented with `[1, 1, 1, 0, 1, 1, 0, 0, 1]`. Two documents are considered similar if their vector representations are similar.\n",
    "\n",
    "In real practice there are many additional techniques to improve the text mining accuracy such as using [stop words](https://en.wikipedia.org/wiki/Stop_words) (i.e. neglecting common words such as `a`, `I`, `to` that don't contribute much meaning), synonym list (e.g. consider `New York City` the same as `NYC` and `Big Apple`), and HTML tag removal if the data sources are webpages. In Module 3 you will learn how to use those advanced techniques for [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing), a component of text mining.\n",
    "\n",
    "In real text mining projects data analysts use packages such as Scikit-Learn and NLTK, which you will learn in Module 3, to extract BoW from texts. In this exercise, however, we would like you to create BoW manually with Python. This is because by manually creating BoW you can better understand the concept and also practice the Python skills you have learned so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZlv0ZHlyqt-"
   },
   "source": [
    "## The Challenge\n",
    "\n",
    "We need to create a BoW from a list of documents. The documents (`doc1.txt`, `doc2.txt`, and `doc3.txt`) can be found in the `your-code` directory of this exercise. You will read the content of each document into an array of strings named `corpus`.\n",
    "\n",
    "*What is a corpus (plural: corpora)? Read the reference in the README file.*\n",
    "\n",
    "Your challenge is to use Python to generate the BoW of these documents. Your BoW should look like below:\n",
    "\n",
    "```python\n",
    "bag_of_words = ['a', 'am', 'at', 'cool', 'i', 'ironhack', 'is', 'love', 'student']\n",
    "\n",
    "term_freq = [\n",
    "    [0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 1, 0, 1, 0],\n",
    "    [1, 1, 1, 0, 1, 1, 0, 0, 1],\n",
    "]\n",
    "```\n",
    "\n",
    "The code below reads the content of a file of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "TlpxS-_e_zmH"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\...doc1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[150]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43m...doc1.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      2\u001b[39m     data_in_file = file.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\javie\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\...doc1.txt'"
     ]
    }
   ],
   "source": [
    "with open('C:\\\\...doc1.txt', 'r') as file:\n",
    "    data_in_file = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlGsnTNu_0XG"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_in_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata_in_file\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'data_in_file' is not defined"
     ]
    }
   ],
   "source": [
    "data_in_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDFTIJVz_4Vp"
   },
   "source": [
    "But Naturally, if we have many files, we don't want to open and read each one explcitly one by one. Let's define the `docs` array that contains the paths of `doc1.txt`, `doc2.txt`, and `doc3.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "C8K6MQQayqt_"
   },
   "outputs": [],
   "source": [
    "docs = ['doc1.txt', 'doc2.txt', 'doc3.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_I0Fcqayqt_"
   },
   "source": [
    "Define an empty array named `corpus` that will contain the content strings of the docs. Loop `docs` and read the content of each doc (see cell above) into the `corpus` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "Uk6N-vogyquA"
   },
   "outputs": [],
   "source": [
    "corpus = [] #Define empty array \n",
    "for data_in_file in docs:\n",
    "    with open(data_in_file, 'r') as file:   #Read the content of each doc\n",
    "        data = file.read()\n",
    "        corpus.append(data) #Append the content to corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPn2JMW_yquA"
   },
   "source": [
    "Print `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "Gg31CafSyquA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ironhack is cool.', 'I love Ironhack.', 'I am a student at Ironhack.']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkzzdrIsyquA"
   },
   "source": [
    "You expected to see:\n",
    "\n",
    "```['ironhack is cool', 'i love ironhack', 'i am a student at ironhack']```\n",
    "\n",
    "But you actually saw:\n",
    "\n",
    "```['Ironhack is cool.', 'I love Ironhack.', 'I am a student at Ironhack.']```\n",
    "\n",
    "This is because you haven't done two important steps:\n",
    "\n",
    "1. Remove punctuation from the strings\n",
    "\n",
    "1. Convert strings to lowercase\n",
    "\n",
    "Write your code below to process `corpus` (convert to lower case and remove special characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "hr19FpCRyquA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ironhack is cool', 'i love ironhack', 'i am a student at ironhack']\n"
     ]
    }
   ],
   "source": [
    "corpus = [] \n",
    "for data_in_file in docs:\n",
    "    with open(data_in_file, 'r') as file:   #Read the content of each doc\n",
    "        data = file.read()\n",
    "        data = data.lower() #Convert to lower case\n",
    "        data = \"\".join([char for char in data if char != \".\"]) #remove special char (punctuation in this case)\n",
    "        corpus.append(data) #Append de content tu corpus\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te53ZNQ5yquA"
   },
   "source": [
    "Now define `bag_of_words` as an empty array. It will be used to store the unique terms in `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "VRpMaq7HyquB"
   },
   "outputs": [],
   "source": [
    "bag_of_words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSjETDxByquB"
   },
   "source": [
    "Loop through `corpus`. In each loop, do the following:\n",
    "\n",
    "1. Break the string into an array of terms.\n",
    "1. Create a sub-loop to iterate the terms array.\n",
    "  * In each sub-loop, you'll check if the current term is already contained in `bag_of_words`. If not in `bag_of_words`, append it to the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH55NCTjyquB"
   },
   "outputs": [],
   "source": [
    "for sentences in corpus:\n",
    "    terms = sentences.split()#split the sentence in terms\n",
    "    for unique_terms in terms:#look for every term and append ir to bag of words\n",
    "        if unique_terms not in bag_of_words:\n",
    "            bag_of_words.append(unique_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucETg_76yquB"
   },
   "source": [
    "Print `bag_of_words`. You should see:\n",
    "\n",
    "```['ironhack', 'is', 'cool', 'i', 'love', 'am', 'a', 'student', 'at']```\n",
    "\n",
    "If not, fix your code in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "RDNezDxvyquB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 'is', 'cool', 'i', 'love', 'am', 'a', 'student', 'at']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZxZ9oCkyquB"
   },
   "source": [
    "Now we define an empty array called `term_freq`. Loop `corpus` for a second time. In each loop, create a sub-loop to iterate the terms in `bag_of_words`. Count how many times each term appears in each doc of `corpus`. Append the term-frequency array to `term_freq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-q_Xw-7yquC"
   },
   "outputs": [],
   "source": [
    "term_freq = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    sentence_terms = sentence.split()#split the sentences in every term\n",
    "    sentence_freq = []#creat empty list to store the frequency of the terms in bag of words\n",
    "    for freq in bag_of_words:\n",
    "        freq_number = sentence_terms.count(freq)#count how many time each word is in every sentence\n",
    "\n",
    "        sentence_freq.append(freq_number)\n",
    "    term_freq.append(sentence_freq)#append the list of the frequency of every word in this empty list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5rTgoo7yquC"
   },
   "source": [
    "Print `term_freq`. You should see:\n",
    "\n",
    "```[[1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 0, 0, 0, 0], [1, 0, 0, 1, 0, 1, 1, 1, 1]]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "63Y_cfsjyquC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 1, 0, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35ESP-61yquC"
   },
   "source": [
    "**If your output is correct, congratulations! You've solved the challenge!**\n",
    "\n",
    "If not, go back and check for errors in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNahUeB4yquC"
   },
   "source": [
    "## Bonus Question\n",
    "\n",
    "Now you want to improve your previous solution by removing the stop words from the corpus. The idea is you only want to add terms that are not in the `stop_words` list to the `bag_of_words` array.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Move all your previous codes from `main.ipynb` to the cell below.\n",
    "1. Improve your solution by ignoring stop words in `bag_of_words`.\n",
    "\n",
    "After you're done, your `bag_of_words` should be:\n",
    "\n",
    "```['ironhack', 'cool', 'love', 'student']```\n",
    "\n",
    "And your `term_freq` should be:\n",
    "\n",
    "```[[1, 1, 0, 0], [1, 0, 1, 0], [1, 0, 0, 1]]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDroiBGYyquC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ironhack is cool', 'i love ironhack', 'i am a student at ironhack']\n",
      "['ironhack', 'cool', 'love', 'student']\n",
      "[[1, 1, 0, 0], [1, 0, 1, 0], [1, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'fifty', 'four', 'not', 'own', 'through', 'yourselves', 'go', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once']\n",
    "docs = ['doc1.txt', 'doc2.txt', 'doc3.txt']\n",
    "\n",
    "corpus = [] \n",
    "for data_in_file in docs:\n",
    "    with open(data_in_file, 'r') as file:   #Read the content of each doc\n",
    "        data = file.read()\n",
    "        data = data.lower() #Convert to lower case\n",
    "        data = \"\".join([char for char in data if char != \".\"]) #remove special char (punctuation in this case)\n",
    "        corpus.append(data) #Append the content tu corpus\n",
    "print(corpus)\n",
    "\n",
    "bag_of_words = []\n",
    "for sentences in corpus:\n",
    "    terms = sentences.split()#Split the sentence in individual terms\n",
    "    for unique_terms in terms:\n",
    "        if unique_terms not in bag_of_words and unique_terms not in stop_words:#look if the word is not in bag of word and also not in stop word\n",
    "            bag_of_words.append(unique_terms)\n",
    "print(bag_of_words)\n",
    "\n",
    "term_freq = []\n",
    "for sentence in corpus:\n",
    "    sentence_terms = sentence.split()\n",
    "    sentence_freq = []\n",
    "    for freq in bag_of_words:# for every term in bag of words, count frequency of appereances\n",
    "        freq_number = sentence_terms.count(freq)\n",
    "\n",
    "        sentence_freq.append(freq_number)\n",
    "    term_freq.append(sentence_freq)\n",
    "print(term_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D0dq58ryquC"
   },
   "source": [
    "## Additional Challenge for the Nerds\n",
    "\n",
    "We will learn Scikit-Learn in Module 3 which has built in the BoW feature. Try to use Scikit-Learn to generate the BoW for this challenge and check whether the output is the same as yours. You will need to do some googling to find out how to use Scikit-Learn to generate BoW.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* To install Scikit-Learn, use `pip install sklearn`.\n",
    "\n",
    "* Scikit-Learn removes stop words by default. You don't need to manually remove stop words.\n",
    "\n",
    "* Scikit-Learn's output has slightly different format from the output example demonstrated above. It's ok, you don't need to convert the Scikit-Learn output.\n",
    "\n",
    "The Scikit-Learn output will look like below:\n",
    "\n",
    "```python\n",
    "# BoW:\n",
    "{u'love': 5, u'ironhack': 3, u'student': 6, u'is': 4, u'cool': 2, u'am': 0, u'at': 1}\n",
    "\n",
    "# term_freq:\n",
    "[[0 0 1 1 1 0 0]\n",
    " [0 0 0 1 0 1 0]\n",
    " [1 1 0 1 0 0 1]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tp_4ILcNyquC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
